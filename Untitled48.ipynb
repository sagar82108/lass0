{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8019700-1371-46ac-9989-f0a1baa29796",
   "metadata": {},
   "outputs": [],
   "source": [
    "R-squared, also known as the coefficient of determination, \n",
    "is a statistical measure used to evaluate the goodness-of-fit of a linear regression model. \n",
    "It quantifies the proportion of the variance in the dependent variable (the variable you're trying to predict)\n",
    "that is explained by the independent variables (the variables used for prediction) in the model.\n",
    " \n",
    "To calculate R-squared, you can use the following forula\n",
    "\n",
    "R^2=1-(SSR/SST)                                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd9c0d3-28ab-44c0-881e-6b652ae8251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adjusted R-squared adjusts for the number of predictors in the model.\n",
    "It penalizes the inclusion of irrelevant variables by decreasing when additional \n",
    "predictors do not contribute enough explanatory power. Therefore, a higher adjusted \n",
    "R-squared indicates a better model fit, taking into account the trade-off between \n",
    "model complexity and explanatory power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e32891-a191-49bc-a577-0afac3f63034",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adjusted R-squared is useful when comparing regression models with different predictors or when you want to\n",
    "balance model complexity and explanatory power. \n",
    "It helps identify the best-fitting model while considering the trade-off between adding predictors and model simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5047f7-8183-4a3f-b2cc-c6cb018c8c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE (Mean Absolute Error), MSE (Mean Squared Error), and RMSE (Root Mean Square Error) are common metrics in regression analysis:\n",
    "\n",
    "- MAE measures the average absolute difference between predicted and actual values.\n",
    "- MSE measures the average squared difference between predicted and actual values.\n",
    "- RMSE is the square root of MSE and brings the error metric back to the original scale.\n",
    "  \n",
    "Smaller values of these metrics indicate better model performance, with RMSE giving error in the same units as the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af41a4bf-362a-4c2f-b367-adf8c2aafb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE is useful when you want error in the same units as your dependent variable and to penalize large errors.\n",
    "MSE is good for emphasizing large errors but is less interpretable. \n",
    "MAE is robust to outliers and straightforward to interpret. \n",
    "Choose the metric that suits your data and goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75df86b-b8e3-4a35-b603-39519b7c2ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso regularization, or Least Absolute Shrinkage and Selection Operator, \n",
    "is a method used in linear regression to encourage a simpler model with feature\n",
    "selection capabilities by adding a penalty term based on the absolute values of coefficients.\n",
    "It differs from Ridge regularization, which uses a penalty term based on the squared values of coefficients,\n",
    "and is more appropriate when you suspect that many features are irrelevant or redundant and want to simplify \n",
    "the model while retaining predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36930571-aab1-47f1-a3be-b7b38df83eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularized linear models (like Ridge and Lasso) prevent overfitting by adding penalty terms to the cost function. \n",
    "Ridge reduces large coefficient values, and Lasso encourages some coefficients to be exactly zero, simplifying the model. \n",
    "This prevents complex, noisy fits to training data and improves generalization. For example, in house price prediction,\n",
    "regularization helps avoid overfitting by focusing on essential features and reducing sensitivity to minor details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02416c0e-9615-4cc7-b900-e911145b1a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularized linear models have limitations:\n",
    "1. They assume linearity, which may not hold in nonlinear relationships.\n",
    "2. Feature engineering is crucial; poor choices can impact performance.\n",
    "3. Hyperparameter tuning is required and sensitive.\n",
    "4. They may not fully resolve multicollinearity.\n",
    "5. Sparse models can reduce interpretability.\n",
    "6. Assumptions about data distribution must be met.\n",
    "7. Over-reliance on regularization can't fix poor preprocessing. Choose models carefully based on data characteristics and goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f661b3-87be-4177-b7cc-f2f9bceb5d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "the choice between RMSE and MAE as the better performer depends on the context and priorities of the analysis.\n",
    "RMSE is more sensitive to large errors and outliers, while MAE provides a more balanced view of prediction accuracy. \n",
    "Consider the problem's specific requirements and the impact of errors when selecting the appropriate metric and model.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e91bbb-52bb-4312-8234-1b62e8fb3caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "The choice between Ridge and Lasso regularization depends on whether you prioritize feature selection,\n",
    "multicollinearity management, or feature retention. Careful tuning of the regularization parameters and\n",
    "consideration of the trade-offs are essential for selecting the appropriate regularization method"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
